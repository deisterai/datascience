# Deep Learning With Keras To Predict Customer Churn

**Objective:**
Use Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.


**IBM Watson dataset:**
The dataset includes information about:
Customers who left within the last month: The column is called Churn
Services that each customer has signed up for: phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
Customer account information: how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges
Demographic info about customers: gender, age range, and if they have partners and dependents


**Import Data:**

```
churn_data_raw <- read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")

glimpse(churn_data_raw)

```
![dataset](dataset.png)

## Preprocess Data
We’ll go through a few steps to preprocess the data for ML. First, we “prune” the data, which is nothing more than removing unnecessary columns and rows.

### Prune the data ###

The “customerID” column is a unique identifier for each observation that isn’t needed for modeling. We can de-select this column.
The data has 11 NA values all in the “TotalCharges” column. Because it’s such a small percentage of the total population (99.8% complete cases), we can drop these observations with the drop_na() function from tidyr. Note that these may be customers that have not yet been charged, and therefore an alternative is to replace with zero or -99 to segregate this population from the rest.
	
```
# Remove unnecessary data
churn_data_tbl <- churn_data_raw %>%
  select(-customerID) %>%
  drop_na() %>%
  select(Churn, everything())
    
glimpse(churn_data_tbl)
```
To avoid using tydir we use: 

```
# Remove unnecessary data
churn_data_tbl <- churn_data_tbl$customerID <- NULL

glimpse(churn_data_tbl)

```

![Dataset v2](dataset_v2.png)


### Split Into Train/Test Sets ###

We have a new package, rsample, which is very useful for sampling methods. It has the initial_split() function for splitting data sets into training and testing sets. The return is a special rsplit object.

```
# Split test/training sets
set.seed(100)
train_test_split <- initial_split(churn_data_tbl, prop = 0.8)
train_test_split

```
<5626/1406/7032> --> <80% / 20% / 100%>


set.seed(seed) is R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced.

Retrieve train and test sets

```
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
```

train_tbl train the 80 % of churn_data_tbl

test_tbl test the 20% of the data generated by the set.seed( ) function. 

We make this procedure to train our future ML neural network.

## Exploration: What Transformation Steps Are Needed For ML?

This phase of the analysis is often called exploratory analysis, but basically we are trying to answer the question, “What steps are needed to prepare for ML?” The key concept is knowing what transformations are needed to run the algorithm most effectively. Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered. In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. A full exploratory analysis is not practical in this article. With that said we’ll cover a few tips on transformations that 

### Preprocessing With Recipes ###
A “recipe” is nothing more than a series of steps you would like to perform on the training, testing and/or validation sets. Think of preprocessing data like baking a cake (I’m not a baker but stay with me). The recipe is our steps to make the cake. It doesn’t do anything other than create the playbook for baking.

We use the recipe() function to implement our preprocessing steps. The function takes a familiar object argument, which is a modeling function such as object = Churn ~ . meaning “Churn” is the outcome (aka response, predictor, target) and all other features are predictors. The function also takes the data argument, which gives the “recipe steps” perspective on how to apply during baking (next).

A recipe is not very useful until we add “steps”, which are used to transform the data during baking. The package contains a number of useful “step functions” that can be applied. The entire list of Step Functions can be viewed here. For our model, we use:

1. step_discretize() with the option = list(cuts = 6) to cut the continuous variable for “tenure” (number of years as a customer) to group customers into cohorts.

2. step_log() to log transform “TotalCharges”.

3. step_dummy() to one-hot encode the categorical data. Note that this adds columns of one/zero for categorical data with three or more categories.

4. step_center() to mean-center the data.

5. step_scale() to scale the data.

**In order to avoid using the recipe library, we will show how to achieve the same steps that a recipe transforms as shown in this next steps.**

### Discretize the “tenure” feature ###

Numeric features like age, years worked, length of time in a position can generalize a group (or cohort). We see this in marketing a lot (think “millennials”, which identifies a group born in a certain timeframe). The “tenure” feature falls into this category of numeric features that can be discretized into groups.


### Transform the “TotalCharges” feature ###

What we don’t like to see is when a lot of observations are bunched within a small part of the range.

![TotalCharges](TotalCharges.png)

We can use a log transformation to even out the data into more of a normal distribution. It’s not perfect, but it’s quick and easy to get our data spread out a bit more.

![log(TotalCharges)](log(TotalCharges).png)

_Pro Tip: A quick test is to see if the log transformation increases the magnitude of the correlation between “TotalCharges” and “Churn”. We’ll use a few dplyr operations along with the corrr package to perform a quick correlation._ 

* correlate( ): Performs tidy correlations on numeric data
* focus( ): Similar to select(). Takes columns and focuses on only the rows/columns of importance.
* fashion( ): Makes the formatting aesthetically easier to read.

```
# Determine if log transformation improves correlation between TotalCharges and Churn
train_tbl %>%
  select(Churn, TotalCharges) %>%
  mutate(
      Churn = Churn %>% as.factor() %>% as.numeric(),
      LogTotalCharges = log(TotalCharges)
      ) %>%
  correlate() %>%
  focus(Churn) %>%
  fashion()
  
```
Result:

|      rowname      |      churn    |
|-------------------|:-------------:|
| 1    TotalCharges |      -20      | 
| 2 LogTotalCharges |      -25      |


Using the _receipes_ package we can perform the step_log( ) function to TotalCharges.

```
step_log() to log transform “TotalCharges”.
  
```

To avoid using the _receipes_ package, we will perform the log transformation as shown below:

```
dat = read.csv("CustomerChurn.csv", header = TRUE)
tc <- (dat$TotalCharges)

tcLog <- log(dat$TotalCharges)

```

### One-hot encoding / Dummy variables ###

One-hot encoding is the process of converting categorical data to sparse data, which has columns of only zeros and ones (this is also called creating “dummy variables” or a “design matrix”). All non-numeric data will need to be converted to dummy variables. This is simple for binary Yes/No data because we can simply convert to 1’s and 0’s. It becomes slightly more complicated with multiple categories, which requires creating new columns of 1’s and 0`s for each category (actually one less). We have four features that are multi-category: 

* Contract
* Internet Service 
* Multiple Lines
* Payment Method

Using the _receipes_ package we can perform the step_dummy( ) function to our multi-categories.

```
step_dummy() to one-hot encode the categorical data. 
  
```

Alternatively we can perform the code that precedes to one-hot encode our categorical data:

```
step_dummy() to one-hot encode the categorical data. 
  
```

## Master document

Tutorial (<https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/>)

